{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3\n",
    "# Learn the basics of Apache Spark in Watson Studio Notebooks\n",
    "\n",
    "This notebook introduces you to the basics of analytics notebooks and explains what Apache Spark is and how to use Spark in notebooks.   \n",
    "The notebook shows you how to load data into the notebook, parse and explore the data, run queries on the data to extract information, plot your analysis results, and save your result in Object Storage.\n",
    "\n",
    "This notebooks runs on Python 3.5 with Spark 2.1, and Cloud Object Storage\n",
    "\n",
    "## Table of contents\n",
    "- [What is Apache Spark](#apache_spark)\n",
    "- [Get data over FTP](#data_set)\n",
    "- [Load data](#load_data)\n",
    "- [Access data](#access_data)\n",
    "- [Add header](#add_header)\n",
    "- [Parse data](#parse_data)\n",
    "- [Explore data](#explore_data)\n",
    "- [Use Spark SQL](#use_spark_sql)\n",
    "- [Save results in Object Storage](#save)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing function\n",
    "from datetime import datetime\n",
    "Timings={}\n",
    "def timing(tag,fromTag=None):\n",
    "    Timings[tag]=datetime.now()\n",
    "    if fromTag:\n",
    "        print(\"`{0}` at {1}, elapsed since {2}: {3}\".format(tag,Timings[tag],fromTag,Timings[tag]-Timings[fromTag]))\n",
    "    else:\n",
    "        print(\"`{0}` at {1}\".format(tag,Timings[tag]))\n",
    "timing(\"A_Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"apache_spark\"></a>\n",
    "## What is Apache Spark\n",
    "\n",
    "[Spark](http://spark.apache.org/) is a fast open-source engine for large-scale data processing. It is built for speed and ease of use. Through the advanced DAG execution engine that supports cyclic data flow and in-memory computing, programs can run up to 100 times faster than Hadoop MapReduce in memory, or 10 times faster on disk.\n",
    "\n",
    "Spark consists of multiple components:\n",
    "\n",
    "* __Spark Core__ is the underlying computation engine with the fundamental programming abstraction called __R__ esilient __D__ istributed __D__ atasets (__RDD__ s)\n",
    "* __Spark SQL__ provides a new data abstraction called DataFrames for structured data processing with SQL and domain-specific language\n",
    "* __MLlib__ is a scalable machine learning framework for delivering fast distributed algorithms for mining big data\n",
    "* __Streaming__ leverages Spark's fast scheduling capability to perform real-time analysis on streams of new data\n",
    "* __GraphX__ is the graph processing framework for the analysis of graph structured data\n",
    "\n",
    "In this introductory tutorial example, you will focus on `Spark Core` and `Spark SQL` by using the Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up access to COS\n",
    "\n",
    "In order to get credentials to write to COS, we will use those of an existing file.   \n",
    "\n",
    "Select any file from the files list on the right-side files panel, and use `insert to code`/`Insert Credentials` in the cell below.   \n",
    "\n",
    ">The credentials object that is created for you is given a generic name, such as *credentials_1*. Rename it to `YourCredentials` before you run the cell (i.e. replace `credentials_1 =` with `YourCredentials =`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "YourCredentials = {\n",
    "    'IBM_API_KEY_ID': 'ZATI1oq_TlsWqN-oEz3Wo1IPXWwOkCx0PXV3gj0d5eui',\n",
    "    'IAM_SERVICE_ID': 'iam-ServiceId-521e4bd0-5161-49f0-9e11-474c674a5fe7',\n",
    "    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'BUCKET': 'watstudworkshop-donotdelete-pr-basx79wonvxlys',\n",
    "    'FILE': '201701-citibike-tripdata.csv'\n",
    "    ,'NOTTHISONE': \"Insert YOUR OWN Credentials as `YourCredentials` in the cell above!!!\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup access to COS\n",
    "import sys\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "def __iter__(self): return 0\n",
    "\n",
    "try:\n",
    "    raise Exception(YourCredentials['NOTTHISONE'])\n",
    "except KeyError: pass\n",
    "\n",
    "bucket_name = YourCredentials['BUCKET']\n",
    "object_name = '2017.csv'\n",
    "\n",
    "cos = ibm_boto3.client('s3',\n",
    "    ibm_api_key_id=YourCredentials['IBM_API_KEY_ID'],\n",
    "    ibm_auth_endpoint=YourCredentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=YourCredentials['ENDPOINT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_set\"></a>\n",
    "## Getting data\n",
    "\n",
    "In this example, you will use Apache Spark to analyze weather data collected from weather stations in 2017. This data is provided by the NOAA National Climatic Data Center (NCDC).\n",
    "\n",
    "We will get the `2017.csv.gz` file, which is quite large after unpacking, directly from its source over FTP download.\n",
    "\n",
    "If you were to manually get at the raw data from the NOAA National Climatic Data Center (NCDC), you would:\n",
    "1. Visit the NCDC site at http://www.ncdc.noaa.gov/data-access/quick-links.\n",
    "2. Click **Global Historical Climatology Network-Daily (GHCN-D)**.\n",
    "3. Click **GHCN-Daily FTP Access**.\n",
    "4. Click the `by_year` folder.\n",
    "5. Scroll to the bottom and click **2017.csv.gz** to download the data set [2017.csv.gz](ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2017.csv.gz)\n",
    "6. After the file was downloaded, extract it.\n",
    "7. Click **ghcn-daily-by_year-format.rtf** to download additional infromation, e.g. header, for the data set. (ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/ghcn-daily-by_year-format.rtf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the data file over FTP\n",
    "If the file is not found in COS storage, we will first get it directly from its original FTP source, by escaping to the underlying linux virtual machine.   \n",
    "Linux commands can be issued using the `!` prefix.\n",
    "\n",
    "We use `wget` and `gunzip` commands.\n",
    "\n",
    "> NOTE: This set of cells needs to be executed only once. After the file is stored into COS, it will be fetched from there directly by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"B_BeforeFTPAndCOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file via FTP download\n",
    "!rm 2017.csv*\n",
    "!wget ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2017.csv.gz\n",
    "!ls -l 2017*\n",
    "# Unzip the file\n",
    "!gunzip -f 2017.csv.gz\n",
    "!ls -l 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the downloaded and unzipped file to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the file to COS from file system\n",
    "with open(object_name, \"rb\") as f:\n",
    "    cos.upload_fileobj(f,bucket_name,object_name)\n",
    "\n",
    "print(\"Done, file {0} uploaded to COS bucket {1}\".format(object_name,bucket_name))\n",
    "timing(\"B_AfterFTPAndCOS\",\"B_BeforeFTPAndCOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you could at this stage go back to the project's Data Assets section and add the file from COS as a Data Asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_data\"></a>\n",
    "## Load data into Spark from COS\n",
    "We will now load the CSV file from COS storage into the notebook's Spark environment\n",
    "<a id=\"access_data\"></a>\n",
    "## Access data\n",
    "Before you can access data in the data file in the Object Storage, you must setup the Spark configuration with your Object Storage credentials. \n",
    "\n",
    "Note that the COS credentials have already been added above. The cell below just copies them to a data structure appropriate for Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark\n",
    "\n",
    "timing(\"C_BeforeDownloadingFromCOS\")\n",
    "\n",
    "# Create credentials in the format required by CloudObjectStorage\n",
    "credentials = {\n",
    "    'endpoint': YourCredentials['ENDPOINT'],\n",
    "    'api_key': YourCredentials['IBM_API_KEY_ID'],\n",
    "    'service_id': YourCredentials['IAM_SERVICE_ID']\n",
    "}\n",
    "\n",
    "configuration_name = 'wheather_data_2017_config_os'\n",
    "# See https://github.com/ibm-watson-data-lab/ibmos2spark/tree/master/python\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials,\n",
    "                                    configuration_name=configuration_name,\n",
    "                                    cos_type='bluemix_cos')\n",
    "\n",
    "# The `sc` object is your SparkContext object\n",
    "# The `cos` object will provide the URL for SparkContext to retrieve your data\n",
    "\n",
    "# Get the URL\n",
    "data_url = cos.url(object_name, bucket_name)\n",
    "timing(\"C_AfterDownloadingFromCOS\",\"C_BeforeDownloadingFromCOS\")\n",
    "\n",
    "print(\"COS URL for the data asset: {0}\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the base file has the following format:\n",
    "\n",
    "<table border=\"1\" style=\"width:90%\">\n",
    "  <tr>\n",
    "    <th>STATION</th><th>DATE</th><th>METRIC</th><th>VALUE</th><th>C5</th><th>C6</th><th>C7</th><th>C8</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>US1NCBC0113</td><td>20170101</td><td>PRCP</td><td>5</td><td></td><td></td><td>N</td><td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CA1MB000296</td><td>20170101</td><td>PRCP</td><td>0</td><td></td><td></td><td>N</td><td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>US1MTMH0019</td><td>20170101</td><td>SNOW</td><td>28</td><td></td><td></td><td>N</td><td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>USW00024229</td><td>20170101</td><td>TMAX</td><td>44</td><td></td><td></td><td>W</td><td>2400</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p>\n",
    "\n",
    "Each row contains a weather station identifier, a date, a metric which is collected (like precipitation, daily maximum and minimum temperatures, temperature at the time of observation, snowfall, snow depth, and so on) and some additional values.\n",
    "\n",
    "**Note**: The header is not included in the CSV file and is documented in **ghcn-daily-by_year-format.rtf**. It was added to the table above to provide clarity on what information each column contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data into a `Spark RDD` we will call `weather` by using the `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"D_BeforeLoadingData\")\n",
    "# Now we load the data. Note that Spark uses lazy evaluation, so the lengthy operation will be take()\n",
    "weather = sc.textFile(data_url)\n",
    "# take() triggers the Spark job that loads the data, See the Spark Job Progress gauge\n",
    "weather.take(5)\n",
    "timing(\"D_AfterLoadingData\",\"D_BeforeLoadingData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now access the data by using the preconfigured `SparkContext` function in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD you created is a collection of strings corresponding to the individual lines in the raw data file. It is also important to remember that the RDD is defined but not instantiated. By applying an action like `count` to the RDD, you effectively instantiate the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total records in the data set: {0}\".format(weather.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply another action to the same RDD that reads the first row of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first row in the data set: {0}\".format(weather.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"add_header\"></a>\n",
    "## Add header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the header is not included in the CSV file, it has to be added programmatically.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RDD from Python array to represent Header (see https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.parallelize)\n",
    "header = sc.parallelize(['STATION,DATE,METRIC,VALUE,C5,C6,C7,C8'])\n",
    "# Append the header and data into a single RDD\n",
    "weather = header.union(weather)\n",
    "weather.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parse_data\"></a>\n",
    "## Parse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To really begin working with the data, you need to parse it into columns. You can do this by mapping each line in the RDD to a function that splits the line by commas.\n",
    "\n",
    "The lambda notation in Python is used to create anonymous functions, in other words, functions which are not bound to a name. This concept is used in the previous code cell to pass a function as a parameter to the `map` function. The anonymous function receives each line from the `weather` RDD  and splits it at comma boundaries. As a result, the new `weatherParse` RDD is defined as a list of lists. Each list in `weatherParse` corresponds to a line in `weather`, and the strings in each list are the individual elements of the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"E_BeforeParsing\")\n",
    "weatherParse = weather.map(lambda line : line.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell for a quick look at the first list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weatherParse.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the individual elements of this first list where the first entry starts at offset zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weatherParse.first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pull elements by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherParse.first()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"E_AfterParsing\",\"E_BeforeParsing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore_data\"></a>\n",
    "## Explore data\n",
    "To better consume the precipitation data, it has to be converted or mapped from one raw form into another format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average precipitation by weather station\n",
    "To calculate the average precipitation that was recorded at each weather station in the data set, reduce the data set by selecting only those rows with precipitation data values, in other words, those rows where the `METRIC` column at index 2 holds the `PRCP` value.   \n",
    "See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter for information on the `filter()` RDD function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new RDD which holds only rows which represent precipitation events\n",
    "weatherPrecp = weatherParse.filter(lambda x: x[2] == \"PRCP\")\n",
    "# Display first 3 lines\n",
    "weatherPrecp.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the `weatherPrecp` RDD that you created contains the subset of weather observations that are precipitation values.   The first column at index 0 is the `Station identifier`, index [1] is the observation date, and index[3] is the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will transform `map` this data set into a new one with a new structure, where each row will retain only the station ID, associated with a pair made of the precipitation value augmented by a new column with the value `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to tuples with station ID as first element, then a tuple made of the precipitation measure and a constant 1\n",
    "# x[0] is the station, x[3] is the precipitation value\n",
    "weatherPrecpCountByKey = weatherPrecp.map(lambda x : (x[0], (int(x[3]), 1)))\n",
    "weatherPrecpCountByKey.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, this results in a RDD with a structure as shown in Table 2:\n",
    "#### Table 2.\n",
    "\n",
    "<table border=\"1\" style=\"width:80%\" align=\"left\">\n",
    "  <tr>\n",
    "    <th>Key</th><th>Value</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Station 1</td><td>(Value 1,1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Station 2</td><td>(Value 2,1)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Station 1</td><td>(Value 3,1)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Station 2</td><td>(Value 4,1)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Station 3</td><td>(Value 5,1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>...</td><td>...</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using this table, you can compute the average precipitation for each station by dividing the summation of the values by the corresponding count, so as to reduce the table into the form represented by Table 3:\n",
    "#### Table 3.\n",
    "\n",
    "<table border=\"1\" style=\"width:80%\" align=\"left\">\n",
    "  <tr>\n",
    "    <th>Key</th><th>Value</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Station 1</td><td>(Value 1 + Value 3,2)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Station 2</td><td>(Value 2 + Value 4,2)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Station 3</td><td>(Value 5,1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>...</td><td>...</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"F_AfterFiltering\",\"E_AfterParsing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the total precipitation by weather station\n",
    "\n",
    "To calculate the total precipitation by weather station, sum (reduce) the precipitation amounts and total readings for every station. We use the `reduceByKey` function for this purpose (see https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)   \n",
    "`reduceByKey` will call the `lambda` function for each pair of rows to coallesce, passing the Value column of the rows in v1 and v2.   \n",
    "We will create a result set RDD format which adds the precipitation values (index[0] of the pair) and count of observations (per station) at index[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherPrecpAddByKey = weatherPrecpCountByKey.reduceByKey(lambda v1,v2 : (v1[0]+v2[0], v1[1]+v2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `first` function, you can inspect the precipitation values and read the totals for the first station ID. Note that this operation might take some time to complete as the whole chain of RDDs that you created are reinstantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherPrecpAddByKey.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average values per station\n",
    "\n",
    "Now that you have transformed the data into the format you need it in, you can finally compute the average precipitation values per weather station. You create the `weatherAverages` RDD by mapping the `weatherPrecpAddByKey` RDD through a function that divides the precipitation total by the total number of readings.   \n",
    "Here, `k` defined as the lambda parameter is a pair, its first value (at index[0] is the Station ID, its second one (at index[1]) is another pair holding the precipitation value and the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherAverages = weatherPrecpAddByKey.map(lambda k: (k[0], k[1][0] / float(k[1][1] ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherAverages.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first ten stations and their average precipitation values. The station ID is the sort order in the `top` function (See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top) because it appears first in the tuple (station ID, average precipitation) in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in weatherAverages.top(10):\n",
    "    print(\"Station {0} had average precipitations of {1: >8.2f}\".format(pair[0],pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to output the ten weather stations with the highest average precipitation, you need to reverse the order of the tuple to (average precipitation, station ID). You can do this with a `map` function that switches the pair order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precTop10=[]\n",
    "stationsTop10=[]\n",
    "for pair in weatherAverages.map(lambda p : (p[1],p[0])).top(10):\n",
    "    precTop10.append(pair[0])\n",
    "    stationsTop10.append(pair[1])\n",
    "    print(\"Station {0} had average precipitations of {1: >8.2f}\".format(pair[1],pair[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will plot the results using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"G_AfterAverages\",\"F_AfterFiltering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 10\n",
    "index = np.arange(N)  \n",
    "bar_width = 0.5\n",
    "\n",
    "plt.bar(index, precTop10, bar_width,color='b')\n",
    "plt.xlabel('Stations')\n",
    "plt.ylabel('Precipitations')\n",
    "plt.title('10 stations with the highest average precipitation')\n",
    "plt.xticks(index + bar_width, stationsTop10, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use_spark_sql\"></a>\n",
    "## Use Spark SQL\n",
    "\n",
    "`Spark SQL` lets you query structured data, for example, data in a relational table and can be a very powerful tool for performing complex aggregations.\n",
    "\n",
    "To create a relational table that you can query using `Spark SQL` and fill it with snowfall data, you'll use the `Row` class from the `pyspark.sql` package. You will use every line in the `weatherSnow` RDD to create a row object. Each of the row's attributes will be used to access the value of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"H_BeginSparkSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter the weather data to show only those weather stations that contain the keyword `SNOW`, you need to reduce the data set to lines with `SNOW` in the third (index 2) column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter where type is snow\n",
    "weatherSnow = weatherParse.filter(lambda x: x[2]==\"SNOW\")\n",
    "print(\"There are {0} SNOW events\".format(weatherSnow.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next commands convert each line of the `weatherSnow` RDD into a SparkSQL `Row` object.  \n",
    "Each row is parsed from the original RDD columns and converted to a tuple with identified types and names.   \n",
    "This allows to infer and apply a schema for the Spark DataFrame built from the collection of Rows   \n",
    "Finally register with the table name `snow2017`, this will allow to run SQL queries on the SparkSQL DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"I_BeginBuildSparkSQLDataFrame\")\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Convert each line of snowWeather RDD into a Row object\n",
    "snowRows= weatherSnow.map(lambda p: Row(station=p[0], month=datetime.strptime(p[1], '%Y%m%d').month, date=datetime.strptime(p[1], '%Y%m%d').day,metric=p[2], value=int(p[3])))\n",
    "\n",
    "# Apply Row schema to create a Spark DataFrame\n",
    "snowSchema = spark.createDataFrame(snowRows)\n",
    "\n",
    "# Register 'snow2017' table with 5 columns: station, month, date, metric, and value\n",
    "snowSchema.registerTempTable(\"snow2017\")\n",
    "timing(\"I_EndBuildSparkSQLDataFrame\",\"I_BeginBuildSparkSQLDataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a Spark DataFrame on which it will be easier to run SQL queries\n",
    "\n",
    "### Compare the number of snow days between two stations\n",
    "\n",
    "In this section, you'll calculate the number of snow days for each month of the year at the `US10chey021` \n",
    "and `USW00094985` weather stations. With that information, you'll plot a bar chart to compare the number of snow days for each month at the two stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find out on how many days of every month it snowed at the `US10chey021` weather station, using a Spark-SQL query, and display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"J_BeginCountSnowDays\")\n",
    "snow_US10chey021 = spark.sql(\"SELECT month, COUNT(*) AS snowdays FROM snow2017 WHERE station='US10chey021' GROUP BY month ORDER BY month\").collect()\n",
    "timing(\"J_EndCountSnowDays\",\"J_BeginCountSnowDays\")\n",
    "snow_US10chey021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have a list of Rows with the aggregation of snowdays for each month when there has been snow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a python array of 12 elements initialized to 0\n",
    "US10chey021_snowdays_y=[0] * 12\n",
    "# fill-in array with snow days per month (notice the 0-indexed array versus 1-indexed months ranks)\n",
    "for row in snow_US10chey021:\n",
    "    US10chey021_snowdays_y[row.month - 1]=row.snowdays\n",
    "    \n",
    "print(\"Snow days per month: \",US10chey021_snowdays_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, find out how many days of every month it snowed at the `USW00094985` weather station and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"K_BeginCountSnowDays\")\n",
    "snow_USW00094985 = spark.sql(\"SELECT  month, COUNT(*) AS snowdays FROM snow2017 WHERE station='USW00094985' GROUP BY month ORDER BY month\").collect()\n",
    "timing(\"K_EndCountSnowDays\",\"K_BeginCountSnowDays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of 12 0 to start with\n",
    "USW00094985_snowdays_y=[0] * 12\n",
    "\n",
    "# For each row, compute number of snow days\n",
    "for row in snow_USW00094985:\n",
    "    USW00094985_snowdays_y[row.month -1]=row.snowdays\n",
    "    \n",
    "print(USW00094985_snowdays_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N=12\n",
    "ind=np.arange(N)\n",
    "width = 0.35\n",
    "pUS10chey021 = plt.bar(ind, US10chey021_snowdays_y, width, color='g', label='US10chey021')\n",
    "pUSW00094985 = plt.bar(ind+width, USW00094985_snowdays_y, width, color='y', label='USW00094985')\n",
    "\n",
    "plt.ylabel('SNOW DAYS')\n",
    "plt.xlabel('MONTH')\n",
    "plt.title('Snow Days in 2017 at Stations US10chey021 vs. USW00094985')\n",
    "plt.xticks(ind+width, ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the number of snow days at each US weather station\n",
    "To determine how many snow days there were at each of the US weather stations in 2017, run the following command to query the `snow2017` table, using the `COUNT(*)` function to get the total snow days. The table is sorted by station name and limited to only 100 stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"L_BeginCountSnowDays\")\n",
    "snowStations = spark.sql(\"SELECT  station, COUNT(*) AS snowdays FROM snow2017 WHERE station LIKE 'US%' GROUP BY station ORDER BY station LIMIT 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 5 rows of the `snowStations` table, including the station name and number of snow days at that station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowStations.head(5)\n",
    "timing(\"L_EndCountSnowDays\",\"L_BeginCountSnowDays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the query results into a new table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will save the query result of the above `SELECT` query in a new table called `snowdays_2017`. The new table has two columns: STATION (name of station) and SNOWDAYS (number of snow days at the station). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowStations.registerTempTable('snowdays_2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the five stations with the highest number of snow days in 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowStations_top5 = spark.sql(\"SELECT station, snowdays FROM snowdays_2017 ORDER BY snowdays DESC LIMIT 5\").collect()\n",
    "for row in snowStations_top5:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the stations with the same number of snow days in 2017\n",
    "\n",
    "In this section, you'll identify the stations that had the same number of snow days in 2017. First, you need to query the table `snowdays_2017` again. \n",
    "\n",
    "Then, by using the `map` function, you'll transform each row into a key-value pair where the key is the number of snow days and the value is the station name.\n",
    "\n",
    "Next, you'll apply the `reduceByKey` function to each pair, and the value of those pairs that have the same key are concatenated. As a result, the RDD contains the number of snow days, and the list of all stations that have the same number of snow days.\n",
    "\n",
    "The `for` loop prints the number of snow days, and the list of stations that have that number of snow days in 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the station's snowdays\n",
    "station_snowdays = spark.sql(\"SELECT station, snowdays FROM snowdays_2017 ORDER BY snowdays\")\n",
    "\n",
    "# Make a RDD with snowdays as first column, used as key\n",
    "snowday_station=station_snowdays.rdd.map(lambda x:  (x.snowdays,x.station))\n",
    "\n",
    "# Collapse by key (snowdays) and make a list of stations as second column\n",
    "snowday_stationsList=snowday_station.reduceByKey(lambda x, y: x + ',' + y).sortByKey().collect()\n",
    "for snowday in snowday_stationsList:\n",
    "    print('Snow days: {0} Stations: {1}'.format(snowday[0],snowday[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing(\"M_EndSparkSQLQueries\",\"H_BeginSparkSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## Save results in Object Storage\n",
    "\n",
    "In this section, you'll save the `snowStations` DataFrame, which is the query result of the `snow2017` table for 100 US weather stations in Object Storage. Each row contains the name of the weather station and the number of snow days at that station.\n",
    "\n",
    "The data will be saved in [Apache Parquet](https://parquet.apache.org/documentation/latest/) file format, which saves data as columns.\n",
    "\n",
    "Each project you create has a container in your object storage. The name of the container is the same as the project name, minus any blank spaces. You can get the name of the container a couple of different ways:\n",
    "1. from the **Settings** page of the project\n",
    "2. or using `YourCredentials['container']` variable that is generated from the **Insert Credentials** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet file \n",
    "\n",
    "# If you are running this cell multiple times, you will need to overwrite the data in the parquet file:\n",
    "#     snowStations.write.mode('overwrite').parquet(bmos.url('CONTAINER', 'snowStations.parquet'))\n",
    "snowStations_filename='snowStations_{0}.parquet'.format(int(datetime.now().timestamp()))\n",
    "snowStations_url = cos.url(snowStations_filename,bucket_name)\n",
    "print(\"Storing snow stations filt to COS file \",snowStations_filename)\n",
    "snowStations.write.parquet(snowStations_url)\n",
    "timing(\"N_EndWriteToCOS\",\"M_EndSparkSQLQueries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the DataFrame has been saved to Object Storage, you will see a new data source called `snowStations.parquet` in the `Data` pane of the project.   \n",
    "It can further be added as a 'Data Asset' to your project.\n",
    "\n",
    "The last cell below reads the parquet file and registers it as a table again.   \n",
    "Once the parquet file is read, you can register the resulting DataFrame as TempTable, and then run SQL queries on the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowDaysParquetFile = spark.read.parquet(cos.url(snowStations_filename,bucket_name))\n",
    "snowDaysParquetFile.registerTempTable(\"snow_from_parquet\")\n",
    "timing(\"O_ReadFromCOS\",\"N_EndWriteToCOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display structure of the DataFrame\n",
    "snowDaysParquetFile.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_snowdays = spark.sql(\"SELECT DISTINCT COUNT(*) AS countSnow FROM snow_from_parquet\")\n",
    "print('There are {0} stations'.format(station_snowdays.first().countSnow))\n",
    "timing(\"P_EndSQLFromCOS\",\"O_ReadFromCOS\")\n",
    "\n",
    "timing(\"Z_End\",\"A_Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "In this sample, you learned how to use Apache Spark, how to use PySpark, the Python API for Spark and the matplotlib plotting library. You also learned how to load data from Object Storage as an RDD, how to define Spark data schemas, and how to save intermediate results in parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"resources\"></a>\n",
    "### Resources\n",
    "- [Apache Spark 2.1.0 Programming Guide](https://spark.apache.org/docs/2.1.0/programming-guide.html)\n",
    "- [Apache Spark 2.1.0 SQL and DataFrames](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html)\n",
    "- [PySpark 2.1.0 - Python API for Spark](https://spark.apache.org/docs/2.1.0/api/python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "Sven Hafeneger is a member of the Data Science Experience development team at IBM in Germany. He holds a M.Sc. in Bioinformatics and is passionate about data analysis, machine learning and the Python ecosystem for data science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright Â© IBM Corp. 2016, 2018. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "name": "cds_ax_spark.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
